<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=ZCOOL+KuaiLe:300,300italic,400,400italic,700,700italic%7CNoto+Sans+SC:300,300italic,400,400italic,700,700italic%7CComfortaa:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"lhmd.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":{"gitalk":{"order":-1}},"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"flipBounceXIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="接触计算机视觉相关的第一门课程。">
<meta property="og:type" content="article">
<meta property="og:title" content="计算机视觉导论">
<meta property="og:url" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/index.html">
<meta property="og:site_name" content="罹魂梦蝶の空間">
<meta property="og:description" content="接触计算机视觉相关的第一门课程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/63.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/64.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/65.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/66.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/67.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/68.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/69.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/70.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/71.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/72.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/73.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/74.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/75.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/76.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/77.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/78.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/79.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/80.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/81.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/82.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/83.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/84.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/85.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/86.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/87.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/88.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/89.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/90.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/91.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/92.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/93.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/94.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/95.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/96.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/97.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/98.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/99.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/100.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/1.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/2.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/3.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/4.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/5.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/6.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/7.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/8.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/9.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/10.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/11.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/12.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/13.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/14.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/15.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/16.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/17.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/18.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/19.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/20.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/21.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/22.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/23.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/24.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/25.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/26.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/27.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/28.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/29.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/30.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/31.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/32.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/33.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/34.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/35.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/36.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/37.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/39.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/38.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/40.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/41.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/42.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/43.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/44.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/45.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/46.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/47.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/48.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/49.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/50.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/51.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/52.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/53.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/54.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/55.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/56.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/57.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/58.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/59.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/60.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/61.png">
<meta property="og:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/62.png">
<meta property="article:published_time" content="2022-11-10T02:08:56.000Z">
<meta property="article:modified_time" content="2023-01-13T07:41:19.740Z">
<meta property="article:author" content="lhmd">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/63.png">


<link rel="canonical" href="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/","path":"2022/11/10/计算机视觉导论/","title":"计算机视觉导论"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>计算机视觉导论 | 罹魂梦蝶の空間</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">罹魂梦蝶の空間</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">6</span></a></li><li class="menu-item menu-item-course"><a href="/categories/course/" rel="section"><i class="fa fa-sitemap fa-fw"></i>course</a></li><li class="menu-item menu-item-勤创"><a href="/categories/%E5%8B%A4%E5%88%9B%E7%9B%B8%E5%85%B3/" rel="section"><i class="fa fa-sitemap fa-fw"></i>勤创</a></li><li class="menu-item menu-item-算法"><a href="/categories/%E7%AE%97%E6%B3%95/" rel="section"><i class="fa fa-sitemap fa-fw"></i>算法</a></li><li class="menu-item menu-item-cv"><a href="/categories/CV/" rel="section"><i class="fa fa-sitemap fa-fw"></i>CV</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-1-Introduction"><span class="nav-text">Lec 1 Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-CV%E4%B8%BB%E8%A6%81%E4%BB%BB%E5%8A%A1%EF%BC%9A"><span class="nav-text">1 CV主要任务：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Review-of-Linear-Algebra"><span class="nav-text">2 Review of Linear Algebra</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-2-Image-formation"><span class="nav-text">Lec 2 Image formation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Camera-and-lens"><span class="nav-text">1 Camera and lens</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Geometric-image-formation"><span class="nav-text">2 Geometric image formation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Photometric-image-formation"><span class="nav-text">3 Photometric image formation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-3-Image-processing"><span class="nav-text">Lec 3 Image processing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Image-processing-basics"><span class="nav-text">1 Image processing basics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Image-sampling"><span class="nav-text">2 Image sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Image-magnification"><span class="nav-text">3 Image magnification</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-4-Model-Fitting-and-Optimization"><span class="nav-text">Lec 4 Model Fitting and Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Optimization"><span class="nav-text">1 Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Numerical-methods"><span class="nav-text">2 Numerical methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Robust-estimation"><span class="nav-text">3 Robust estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Interpolation"><span class="nav-text">4 Interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Graphcut"><span class="nav-text">5 Graphcut</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-5-Image-Matching-and-Motion-Estimation"><span class="nav-text">Lec 5 Image Matching and Motion Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Image-matching"><span class="nav-text">1 Image matching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Motion-estimation"><span class="nav-text">2 Motion estimation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-6-Image-stitching"><span class="nav-text">Lec 6 Image stitching</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Image-Warping"><span class="nav-text">1 Image Warping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Image-Stitching"><span class="nav-text">2 Image Stitching</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-7-Structure-from-Motion"><span class="nav-text">Lec 7 Structure from Motion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Camera-calibration"><span class="nav-text">1 Camera calibration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Two-frame-structure-from-motion"><span class="nav-text">2 Two-frame structure from motion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Multi-frame-structure-from-motion"><span class="nav-text">3 Multi-frame structure from motion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-A-modern-SfM-system-COLMAP"><span class="nav-text">4 A modern SfM system: COLMAP</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-8-Depth-estimation-and-3D-reconstruction"><span class="nav-text">Lec 8 Depth estimation and 3D reconstruction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Depth-estimation"><span class="nav-text">1 Depth estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3D-reconstruction"><span class="nav-text">2 3D reconstruction</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-9-Deep-Learning"><span class="nav-text">Lec 9 Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Machine-learning"><span class="nav-text">1 Machine learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Linear-classifier"><span class="nav-text">2 Linear classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Neural-networks"><span class="nav-text">3 Neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Convolutional-neural-networks%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">4 Convolutional neural networks—卷积神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Training-neural-networks"><span class="nav-text">5 Training neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Network-architectures"><span class="nav-text">6 Network architectures</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-10-Recognition"><span class="nav-text">Lec 10 Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Semantic-segmentation-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="nav-text">1 Semantic segmentation(语义分割)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Object-detection-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-text">2 Object detection(目标检测)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Instance-segmentation-%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="nav-text">3 Instance segmentation(实例分割)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Human-pose-estimation-%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1"><span class="nav-text">4 Human pose estimation(人体姿态估计)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Optical-flow-%E5%85%89%E6%B5%81"><span class="nav-text">5 Optical flow(光流)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Other-tasks"><span class="nav-text">6 Other tasks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-11-3D-Deep-Learning"><span class="nav-text">Lec 11  3D Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Feature-matching"><span class="nav-text">1 Feature matching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Object-Pose-Estimation"><span class="nav-text">2 Object Pose Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Human-Pose-Estimation"><span class="nav-text">3 Human Pose Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Depth-Estimation"><span class="nav-text">4 Depth Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Single-Image-to-3D"><span class="nav-text">5 Single Image to 3D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-PointNet"><span class="nav-text">6 PointNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-12-Computational-Photography-I"><span class="nav-text">Lec 12 Computational Photography I</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-High-Dynamic-Range-Imaging-HDR"><span class="nav-text">1 High Dynamic Range Imaging (HDR)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Deblurring"><span class="nav-text">2 Deblurring</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Colorization-%E9%BB%91%E7%99%BD%E5%8F%98%E5%BD%A9%E8%89%B2"><span class="nav-text">3 Colorization(黑白变彩色)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Super-Resolution"><span class="nav-text">4 Super Resolution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lec-13-Computational-Photography-II"><span class="nav-text">Lec 13 Computational Photography II</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Image-based-Rendering"><span class="nav-text">1 Image-based Rendering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Neural-Rendering"><span class="nav-text">2 Neural Rendering</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lhmd"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">lhmd</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/lhmd" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lhmd" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zjuwjwang@gmail.com" title="E-Mail → mailto:zjuwjwang@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh_CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://unpkg.com/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


      <a target="_blank" rel="noopener" href="https://github.com/lhmd" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lhmd">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="罹魂梦蝶の空間">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="计算机视觉导论 | 罹魂梦蝶の空間">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          计算机视觉导论
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-10 10:08:56" itemprop="dateCreated datePublished" datetime="2022-11-10T10:08:56+08:00">2022-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-13 15:41:19" itemprop="dateModified" datetime="2023-01-13T15:41:19+08:00">2023-01-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CV/courses/" itemprop="url" rel="index"><span itemprop="name">courses</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>32 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>接触计算机视觉相关的第一门课程。</p>
<span id="more"></span>
<h2 id="Lec-1-Introduction"><a href="#Lec-1-Introduction" class="headerlink" title="Lec 1 Introduction"></a>Lec 1 Introduction</h2><h3 id="1-CV主要任务："><a href="#1-CV主要任务：" class="headerlink" title="1 CV主要任务："></a>1 CV主要任务：</h3><ul>
<li>三维重建</li>
<li>图像理解</li>
<li>图像合成</li>
</ul>
<h3 id="2-Review-of-Linear-Algebra"><a href="#2-Review-of-Linear-Algebra" class="headerlink" title="2 Review of Linear Algebra"></a>2 Review of Linear Algebra</h3><p>省略。。。</p>
<h2 id="Lec-2-Image-formation"><a href="#Lec-2-Image-formation" class="headerlink" title="Lec 2 Image formation"></a>Lec 2 Image formation</h2><h3 id="1-Camera-and-lens"><a href="#1-Camera-and-lens" class="headerlink" title="1 Camera and lens"></a>1 Camera and lens</h3><h4 id="Pinhole-camera"><a href="#Pinhole-camera" class="headerlink" title="Pinhole camera"></a>Pinhole camera</h4><p>Add a barrier to block off most of the rays, the opening known as the aperture(光圈).</p>
<h4 id="Lens"><a href="#Lens" class="headerlink" title="Lens"></a>Lens</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/63.png" class>
<p>Focal length:</p>
<script type="math/tex; mode=display">\frac{1}{i}+\frac{1}{o}=\frac{1}{f}</script><p>Image Magnification:</p>
<script type="math/tex; mode=display">m=\frac{h_i}{h_o}</script><p>Field of View (FOV):</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/64.png" class>
<ul>
<li>Longer focal length = Narrower angle of view</li>
<li>Shorter focal length = Wider angle of view </li>
<li>FOV also depends on sensor size</li>
</ul>
<p>Aperture: control image brightness</p>
<p>F-number: represent aperture as a fraction of focal length</p>
<p>Depth of Field: range of object distances over which the image is sufficiently well focused</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/65.png" class>
<h3 id="2-Geometric-image-formation"><a href="#2-Geometric-image-formation" class="headerlink" title="2 Geometric image formation"></a>2 Geometric image formation</h3><h4 id="Pinhole-camera-model-Perspective-Projection"><a href="#Pinhole-camera-model-Perspective-Projection" class="headerlink" title="Pinhole camera model: Perspective Projection"></a>Pinhole camera model: Perspective Projection</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/66.png" class>
<h4 id="Homogeneous-coordinates"><a href="#Homogeneous-coordinates" class="headerlink" title="Homogeneous coordinates"></a>Homogeneous coordinates</h4><ul>
<li>Converting from Cartesian to Homogeneous coordinates(add an extra dimension)</li>
<li>Converting from Homogeneous to Cartesian(remove the last dimension by dividing a number)</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/67.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/68.png" class>
<p>Homogeneous coordinates are invariant to scaling. </p>
<p>Each point has an infinite set of homogeneous coordinates. </p>
<p>The point in the 2D plane is projection of a ray in 3D space:</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/69.png" class>
<h4 id="Perspective-Projection"><a href="#Perspective-Projection" class="headerlink" title="Perspective Projection"></a>Perspective Projection</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/70.png" class>
<h4 id="Orthographic-projection"><a href="#Orthographic-projection" class="headerlink" title="Orthographic projection"></a>Orthographic projection</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/71.png" class>
<h3 id="3-Photometric-image-formation"><a href="#3-Photometric-image-formation" class="headerlink" title="3 Photometric image formation"></a>3 Photometric image formation</h3><h4 id="Shutter-speed"><a href="#Shutter-speed" class="headerlink" title="Shutter speed"></a>Shutter speed</h4><p>Shutter speed controls exposure time. The pixel value is equal to the integral of the light intensity within the exposure time.</p>
<h4 id="Rolling-shutter-effect"><a href="#Rolling-shutter-effect" class="headerlink" title="Rolling shutter effect"></a>Rolling shutter effect</h4><p>Exposing the image line by line.</p>
<h4 id="Color-spaces"><a href="#Color-spaces" class="headerlink" title="Color spaces"></a>Color spaces</h4><ul>
<li>RGB </li>
<li>HSV (Hue / Value / Saturation)</li>
</ul>
<h4 id="Color-Sensing-Bayer-filter"><a href="#Color-Sensing-Bayer-filter" class="headerlink" title="Color Sensing: Bayer filter"></a>Color Sensing: Bayer filter</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/72.png" class>
<h2 id="Lec-3-Image-processing"><a href="#Lec-3-Image-processing" class="headerlink" title="Lec 3 Image processing"></a>Lec 3 Image processing</h2><h3 id="1-Image-processing-basics"><a href="#1-Image-processing-basics" class="headerlink" title="1 Image processing basics"></a>1 Image processing basics</h3><h4 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/73.png" class>
<h4 id="2D-convolution"><a href="#2D-convolution" class="headerlink" title="2D convolution"></a>2D convolution</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/74.png" class>
<h4 id="Discrete-2D-convolution"><a href="#Discrete-2D-convolution" class="headerlink" title="Discrete 2D convolution"></a>Discrete 2D convolution</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/75.png" class>
<p>This can also be the same with the vector/matrix dot product.</p>
<h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>Adding pixels around the image border.</p>
<h4 id="Blur"><a href="#Blur" class="headerlink" title="Blur"></a>Blur</h4><p>box滤波：卷积核全为1</p>
<p>高斯滤波：特定函数</p>
<h4 id="Sharpen"><a href="#Sharpen" class="headerlink" title="Sharpen"></a>Sharpen</h4><p>Sharpening is adding high frequencies. </p>
<ul>
<li>Let be the original image. </li>
<li>High frequencies in image is J-blur(I), blur(I) is the low frequencied in image. </li>
<li>Sharpened image is I+(I-blur(I)).</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/76.png" class>
<h4 id="Gradient-detection-filter"><a href="#Gradient-detection-filter" class="headerlink" title="Gradient detection filter"></a>Gradient detection filter</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/77.png" class>
<h4 id="Bilateral-filter"><a href="#Bilateral-filter" class="headerlink" title="Bilateral filter"></a>Bilateral filter</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/78.png" class>
<h3 id="2-Image-sampling"><a href="#2-Image-sampling" class="headerlink" title="2 Image sampling"></a>2 Image sampling</h3><blockquote>
<p>Change image size / resolution. resolution: pixel / inch</p>
</blockquote>
<h4 id="Reducing-image-size"><a href="#Reducing-image-size" class="headerlink" title="Reducing image size"></a>Reducing image size</h4><p>降采样，去掉旁边的像素或者取均值</p>
<h4 id="Aliasing"><a href="#Aliasing" class="headerlink" title="Aliasing"></a>Aliasing</h4><p>Aliasing - artifacts due to sampling </p>
<p>Signals are changing too fast but sampled too slow.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/79.png" class>
<h4 id="Anti-aliasing"><a href="#Anti-aliasing" class="headerlink" title="Anti-aliasing"></a>Anti-aliasing</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/80.png" class>
<p>How to do anti-aliasing </p>
<ul>
<li>Convolve the image with low-pass filters (e.g. Gaussian). </li>
<li>Sample it with a Nyquist rate.</li>
</ul>
<h4 id="Fourier-Transform"><a href="#Fourier-Transform" class="headerlink" title="Fourier Transform"></a>Fourier Transform</h4><p>Represent a function as a weighted sum of sines and cosines.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/81.png" class>
<h3 id="3-Image-magnification"><a href="#3-Image-magnification" class="headerlink" title="3 Image magnification"></a>3 Image magnification</h3><h4 id="Interpolation"><a href="#Interpolation" class="headerlink" title="Interpolation"></a>Interpolation</h4><ul>
<li><p>Nearest-neighbor interpolation </p>
</li>
<li><p>Linear interpolation </p>
</li>
<li><p>Cubic spline interpolation (Polynomial interpolation) </p>
<p>Each interval of function has different parameters</p>
</li>
<li><p>Bilinear Interpolation (2D) Bilinear Interpolation is good enough.</p>
</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/82.png" class>
<h4 id="Seam-Carving"><a href="#Seam-Carving" class="headerlink" title="Seam Carving"></a>Seam Carving</h4><p>A method to change aspect ratio. </p>
<p>Find connected path of pixels from top to bottom of which the edge energy is minimal, removing unnoticeable pixels.</p>
<p>Algorithm: DP</p>
<h2 id="Lec-4-Model-Fitting-and-Optimization"><a href="#Lec-4-Model-Fitting-and-Optimization" class="headerlink" title="Lec 4 Model Fitting and Optimization"></a>Lec 4 Model Fitting and Optimization</h2><blockquote>
<p>本讲与数值分析课程高度重合</p>
</blockquote>
<h3 id="1-Optimization"><a href="#1-Optimization" class="headerlink" title="1 Optimization"></a>1 Optimization</h3><blockquote>
<p>minimize f~0~(x)</p>
</blockquote>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/83.png" class>
<h4 id="Model-fitting"><a href="#Model-fitting" class="headerlink" title="Model fitting"></a>Model fitting</h4><p>Mean Square Error (MSE)</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/84.png" class>
<h3 id="2-Numerical-methods"><a href="#2-Numerical-methods" class="headerlink" title="2 Numerical methods"></a>2 Numerical methods</h3><p>Find a solution path: F (x~0~) &gt; F (x~1~) &gt; … &gt; F (x~k~) &gt; …</p>
<p>与数值分析课上内容类似，泰勒展开，进行迭代，做一阶、二阶近似</p>
<h4 id="梯度下降法：迭代法的一种"><a href="#梯度下降法：迭代法的一种" class="headerlink" title="梯度下降法：迭代法的一种"></a>梯度下降法：迭代法的一种</h4><h4 id="Newton-method：见数值分析课程"><a href="#Newton-method：见数值分析课程" class="headerlink" title="Newton method：见数值分析课程"></a>Newton method：见数值分析课程</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/85.png" class>
<h4 id="高斯-牛顿迭代"><a href="#高斯-牛顿迭代" class="headerlink" title="高斯-牛顿迭代"></a>高斯-牛顿迭代</h4><h3 id="3-Robust-estimation"><a href="#3-Robust-estimation" class="headerlink" title="3 Robust estimation"></a>3 Robust estimation</h3><p>Use other loss functions to replace MSE，去掉影响很大的噪点</p>
<h4 id="Random-Sample-Concensus-RANSAC"><a href="#Random-Sample-Concensus-RANSAC" class="headerlink" title="Random Sample Concensus (RANSAC)"></a>Random Sample Concensus (RANSAC)</h4><p>Key ideas </p>
<ul>
<li>The distribution of inliers is similar  while outliers differ a lot </li>
<li>Use data point pairs to vote</li>
</ul>
<h3 id="4-Interpolation"><a href="#4-Interpolation" class="headerlink" title="4 Interpolation"></a>4 Interpolation</h3><blockquote>
<p>具体请参考数值分析</p>
</blockquote>
<p>线性插值</p>
<p>三次样条插值</p>
<h3 id="5-Graphcut"><a href="#5-Graphcut" class="headerlink" title="5 Graphcut"></a>5 Graphcut</h3><h4 id="Images-as-Graphs"><a href="#Images-as-Graphs" class="headerlink" title="Images as Graphs"></a>Images as Graphs</h4><p>A vertex for each pixel, an edge between each pair, each edge is weighted by the affinity or similarity between its two vertices.</p>
<h4 id="Normalized-cut"><a href="#Normalized-cut" class="headerlink" title="Normalized cut"></a>Normalized cut</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/86.png" class>
<h4 id="Markov-Random-Field-MRF"><a href="#Markov-Random-Field-MRF" class="headerlink" title="Markov Random Field (MRF)"></a>Markov Random Field (MRF)</h4><p>马尔科夫随机场，没看懂</p>
<h2 id="Lec-5-Image-Matching-and-Motion-Estimation"><a href="#Lec-5-Image-Matching-and-Motion-Estimation" class="headerlink" title="Lec 5 Image Matching and Motion Estimation"></a>Lec 5 Image Matching and Motion Estimation</h2><h3 id="1-Image-matching"><a href="#1-Image-matching" class="headerlink" title="1 Image matching"></a>1 Image matching</h3><blockquote>
<p>Finding point-to-point correspondences between two images.</p>
</blockquote>
<h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><ul>
<li>Detection: Identify the interest points (key points). </li>
<li>Description: Extract vector feature descriptor surrounding each interest point. </li>
<li>Matching: Determine correspondence between descriptors in two views.</li>
</ul>
<h4 id="detection"><a href="#detection" class="headerlink" title="detection"></a>detection</h4><p>Principal Component Analysis (PCA)</p>
<p>角点检测</p>
<ul>
<li><p>Compute the covariance matrix at each point</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/87.png" class>
</li>
<li><p>Compute eigenvalues</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/88.png" class>
</li>
<li><p>Classify points using eigenvalues of H:</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/89.png" class>
</li>
</ul>
<h4 id="Blob-detector"><a href="#Blob-detector" class="headerlink" title="Blob detector"></a>Blob detector</h4><p>Blobs are have large second derivatives in image intensity.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/90.png" class>
<h4 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h4><p>SIFT descriptor：Scale Invariant Feature Transform descriptor, use histogram of oriented gradients.</p>
<p>SIFT Algorithm</p>
<ul>
<li>Run DoG detector to find maximum in location/scale space.</li>
<li>Find dominate orientation and normalize the orientation.</li>
<li>For each (x, y, scale, orientation), create the only descriptor.</li>
</ul>
<h4 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h4><p>Define the difference between two features f1 , f2, distance L2 = ||f1 − f2 ||</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/91.png" class>
<h3 id="2-Motion-estimation"><a href="#2-Motion-estimation" class="headerlink" title="2 Motion estimation"></a>2 Motion estimation</h3><h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>Both feature matching and motion estimation are called correspondence problems.</p>
<p>特征追踪：</p>
<p>Extract feature (interest) points and “track” them over multiple frames. </p>
<p>Output: displacement of sparse points</p>
<p>光流法：</p>
<p>Recover image motion at each pixel </p>
<p>Output: dense displacement field (optical flow filed)</p>
<h4 id="Lucas-Kanade-Method"><a href="#Lucas-Kanade-Method" class="headerlink" title="Lucas-Kanade Method"></a>Lucas-Kanade Method</h4><p>Key assumptions：</p>
<ul>
<li>Small motion: points do not move very far </li>
<li>Brightness constancy: same point looks the same(in brightness) in every frame </li>
<li>Spatial coherence: points move like their neighbors</li>
</ul>
<p>本讲小总结：</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/92.png" class>
<h2 id="Lec-6-Image-stitching"><a href="#Lec-6-Image-stitching" class="headerlink" title="Lec 6 Image stitching"></a>Lec 6 Image stitching</h2><h3 id="1-Image-Warping"><a href="#1-Image-Warping" class="headerlink" title="1 Image Warping"></a>1 Image Warping</h3><p>Change shape of image</p>
<h4 id="Linear-Transformmation"><a href="#Linear-Transformmation" class="headerlink" title="Linear Transformmation"></a>Linear Transformmation</h4><p>Linear map = Matrices</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/93.png" class>
<h4 id="Affine-Transformation"><a href="#Affine-Transformation" class="headerlink" title="Affine Transformation"></a>Affine Transformation</h4><p>Affine map = linear map + translation</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/94.png" class>
<p>Using homogenous coordinates</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/95.png" class>
<ul>
<li><p>6 unknowns in equations </p>
</li>
<li><p>2 equations for each match </p>
</li>
<li><p>we need at least 3 matches to solve a affine transformation </p>
</li>
<li><p>for n matches, solve with least squares</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/96.png" class>
</li>
</ul>
<p>The result of solution must be remembered. </p>
<p>the last row with matrix must be [0 0 1]</p>
<h4 id="Projective-Transformation-Homography"><a href="#Projective-Transformation-Homography" class="headerlink" title="Projective Transformation (Homography)"></a>Projective Transformation (Homography)</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/97.png" class>
<ul>
<li><p>8 unknowns in equations </p>
</li>
<li><p>Homography matrix is up to scale (can be multiplied by a scalar), which means the degree of freedom is 8 . </p>
</li>
<li><p>2 equations for each match </p>
</li>
<li><p>we need at least 4 matches to solve the homography </p>
</li>
<li><p>for n matches, solve with ||h|| = 1</p>
<p>h = eigenvector of A^T^A with smallest eigenvalue</p>
</li>
</ul>
<h4 id="DoF"><a href="#DoF" class="headerlink" title="DoF"></a>DoF</h4><ul>
<li>Translation: The degree of freedom is 2 </li>
<li>Affine: The degree of freedom is 6 </li>
<li>Projective: The degree of freedom is 8</li>
</ul>
<h4 id="Forward-Warping"><a href="#Forward-Warping" class="headerlink" title="Forward Warping"></a>Forward Warping</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/98.png" class>
<h4 id="Inverse-Warping"><a href="#Inverse-Warping" class="headerlink" title="Inverse Warping"></a>Inverse Warping</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/99.png" class>
<p>if pixel lands between pixels, we interpolate color values from neighboring pixels.</p>
<ul>
<li>nearest neighbor </li>
<li>bilinear (usually this method is enough) </li>
<li>bicubic</li>
</ul>
<h3 id="2-Image-Stitching"><a href="#2-Image-Stitching" class="headerlink" title="2 Image Stitching"></a>2 Image Stitching</h3><p>Algorithm:</p>
<ul>
<li>Input images </li>
<li>Feature matching </li>
<li>Compute transformation matrix with RANSAC </li>
<li>Fix image 1 and warp image 2</li>
</ul>
<p>Cylindrical projection:</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/100.png" class>
<h2 id="Lec-7-Structure-from-Motion"><a href="#Lec-7-Structure-from-Motion" class="headerlink" title="Lec 7 Structure from Motion"></a>Lec 7 Structure from Motion</h2><blockquote>
<p>Target: recover camera poses and 3D structure of a scene from its images</p>
</blockquote>
<h3 id="1-Camera-calibration"><a href="#1-Camera-calibration" class="headerlink" title="1 Camera calibration"></a>1 Camera calibration</h3><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><p>参考文献：<a target="_blank" rel="noopener" href="https://blog.csdn.net/fengye2two/article/details/80686409/">图像处理——相机标定</a></p>
<blockquote>
<p>世界坐标系（world coordinate）(<em>xw,yw,zw</em>)，也称为测量坐标系，是一个三维直角坐标系，以其为基准可以描述相机和待测物体的空间位置。世界坐标系的位置可以根据实际情况自由确定。世界坐标系的最小单位为mm。</p>
<p>相机坐标系（camera coordinate）(<em>xc,yc,zc</em>)，也是一个三维直角坐标系，原点位于镜头光心处，xc、yc轴分别与像面的两边平行，zc轴为镜头光轴，与像平面垂直。相机坐标系的最小单位为mm。</p>
<p>图像坐标系（image coordinate）(<em>x</em>,<em>y</em>)，是像平面上的二维直角坐标系。图像坐标系的原点为镜头光轴与像平面的交点（也称主点，principal point），它的x轴与相机坐标系的xc轴平行，它的y轴与相机坐标系的yc轴平行。图像坐标系的最小单位为mm。</p>
<p>像素坐标系（pixel coordinate）(u,v)，是图像处理工作中常用的二维直角坐标系，反映了相机CCD/CMOS芯片中像素的排列情况。它的原点位于图像左上角，横坐标u表示像素所在的列，纵坐标v表示像素所在的行。像素坐标系与图像坐标系可以简单理解为平移关系，它们同处于像平面。像素坐标系的x轴与图像坐标系的u轴平行，像素坐标系的y轴与图像坐标系的v轴平行。像素坐标系的最小单位为像素。</p>
</blockquote>
<p>变换过程：</p>
<p>世界=》相机=》图像=》像素</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/1.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/2.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/3.png" class>
<p>so, it is similar to lab2.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/4.png" class>
<p>世界直接转换为像素：</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/5.png" class>
<p>解方程时：</p>
<ul>
<li>找特征点，建立方程求解未知数$p$</li>
</ul>
<blockquote>
<p>具体查看参考文献和课程PPT</p>
</blockquote>
<h4 id="PnP问题"><a href="#PnP问题" class="headerlink" title="PnP问题"></a>PnP问题</h4><p>参考文献：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/399140251">PnP问题各种算法总结分析</a></p>
<blockquote>
<p>问题描述：已知n个3D点的坐标(相对世界坐标系)以及这些点的像素坐标时，如何估计相机的位姿</p>
</blockquote>
<h5 id="Direct-Linear-Transform-DLT"><a href="#Direct-Linear-Transform-DLT" class="headerlink" title="Direct Linear Transform (DLT)"></a>Direct Linear Transform (DLT)</h5><p>前面我们通过解方程的形式解出了这个方程，这种方法就叫做DLT。</p>
<h5 id="P3P"><a href="#P3P" class="headerlink" title="P3P"></a>P3P</h5><p>至少三个对应关系可以解出相机坐标，还需要一个对应关系使这个解是特解。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/6.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/7.png" class>
<h5 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h5><p>Main steps: </p>
<ol>
<li>Represent each point as the linear combination of 4 control points c~i~. </li>
<li><p>Construct a linear system in the control-point coordinate.</p>
</li>
<li><p>Solve the equation.</p>
</li>
</ol>
<h3 id="2-Two-frame-structure-from-motion"><a href="#2-Two-frame-structure-from-motion" class="headerlink" title="2 Two-frame structure from motion"></a>2 Two-frame structure from motion</h3><ol>
<li>Assume Camera Matrix 𝐾 is known for each camera </li>
<li>Find a few Reliable Corresponding Points</li>
<li>Find Relative Camera Position 𝐭 and Orientation 𝑅</li>
<li>Find 3D position of scene points</li>
</ol>
<p>详细讲解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/472205819">对极几何—知乎</a></p>
<p>​                    <a target="_blank" rel="noopener" href="https://xhy3054.github.io/epipolar-geometry/">对极几何—github</a></p>
<h3 id="3-Multi-frame-structure-from-motion"><a href="#3-Multi-frame-structure-from-motion" class="headerlink" title="3 Multi-frame structure from motion"></a>3 Multi-frame structure from motion</h3><ol>
<li>Initialize camera motion and scene structure </li>
<li>For each additional view - Determine projection matrix of new camera using all the<br> known 3D points that are visible in its image - Refine and extend structure: compute new 3D points, reoptimize existing points that are also seen by this camera</li>
<li>Refine structure and motion: Bundle Adjustment</li>
</ol>
<h3 id="4-A-modern-SfM-system-COLMAP"><a href="#4-A-modern-SfM-system-COLMAP" class="headerlink" title="4 A modern SfM system: COLMAP"></a>4 A modern SfM system: COLMAP</h3><blockquote>
<p>sfM: Structure-from-Motion</p>
<p>MVS: Multi-View Stereo</p>
</blockquote>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/8.png" class>
<h2 id="Lec-8-Depth-estimation-and-3D-reconstruction"><a href="#Lec-8-Depth-estimation-and-3D-reconstruction" class="headerlink" title="Lec 8 Depth estimation and 3D reconstruction"></a>Lec 8 Depth estimation and 3D reconstruction</h2><h3 id="1-Depth-estimation"><a href="#1-Depth-estimation" class="headerlink" title="1 Depth estimation"></a>1 Depth estimation</h3><h4 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h4><p>​    深度传感器顾名思义是用来探测环境物体与传感器之间的距离的。它的输出主要可以表示为深度图(depth map)和点云(point cloud)这两种形式。</p>
<p>​    深度图像（depth image)也被称为距离影像（range image），是指将从图像采集器到场景中各点的距离（深度）作为像素值的图像，它直接反映了景物可见表面的几何形状。深度图像经过坐标转换可以计算为点云数据，有规则及必要信息的点云数据也可以反算为深度图像数据。<br>深度数据流所提供的图像帧中，每一个像素点代表的是在深度感应器的视野中，该特定的（x, y）坐标处物体到离摄像头平面最近的物体到该平面的距离（以毫米为单位）。</p>
<ul>
<li>被动测距传感(Passive depth sensing)</li>
</ul>
<blockquote>
<p>被动测距传感=两个相隔一定距离的相机获得两幅图像+立体匹配+三角原理计算视差（disparity）</p>
</blockquote>
<p>​        两个相隔一定距离的摄像机同时获取同一场景的两幅图像，通过立体匹配算法找到两幅图像中对应的像素点，随后根据三角原理计算出视差信息，而视差信息通过转换可用于表征场景中物体的深度信息。基于立体匹配算法，还可通过拍摄同一场景下不同角度的一组图像来获得该场景的深度图像。除此之外，场景深度信息还可以通过对图像的光度特征、明暗特征等特征进行分析间接估算得到。</p>
<ul>
<li>主动测距传感(Active depth sensing)</li>
</ul>
<p>​        主动测距传感相比较于被动测距传感最明显的特征是：设备本身需要发射能量来完成深度信息的采集。这也就保证了深度图像的获取独立于彩色图像的获取。近年来，主动深度传感在市面上的应用愈加丰富。主动深度传感的方法主要包括了TOF（Time of Flight）、结构光、激光扫描等。</p>
<h4 id="1-2-Stereo-matching"><a href="#1-2-Stereo-matching" class="headerlink" title="1.2 Stereo matching"></a>1.2 Stereo matching</h4><blockquote>
<p>参考资料<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161276985">3D视觉之立体匹配</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Android_WPF/article/details/126434543">立体匹配算法</a></p>
</blockquote>
<p>最简单的算法：</p>
<ul>
<li>For each pixel in the first image <ul>
<li>Find corresponding epipolar line in the right image</li>
<li>Search along epipolar line and pick the best match</li>
</ul>
</li>
<li>Simplest case: epipolar lines are horizontal scanlines</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/9.png" class>
<p>这样就找到了两个相同的点，然后计算深度。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/10.png" class>
<p>如果视角不在同一水平线上，就先把他们转到同一水平线。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/11.png" class>
<p>Stereo as energy minimization：让当前像素的代价聚合过程受多个方向(或路径)上所有像素的影响，方向越多参与影响当前像素的邻域像素就越多</p>
<p>动态规划：</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/12.png" class>
<p>Choosing the stereo baseline：</p>
<ul>
<li>Too small: large depth error </li>
<li>Too large: difficult search problem</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/13.png" class>
<h4 id="1-3-Multi-view-stereo"><a href="#1-3-Multi-view-stereo" class="headerlink" title="1.3 Multi-view stereo"></a>1.3 Multi-view stereo</h4><p>Plane-Sweep: <a target="_blank" rel="noopener" href="https://blog.csdn.net/xuangenihao/article/details/81392684">平面扫描算法</a></p>
<p>PatchMatch: <a href="PatchMatch">PatchMatch</a></p>
<ol>
<li>Initialize pixels with random patch offsets</li>
<li>Check if neighbors have better patch offsets</li>
<li>Search in concentric radius around the current offset for better better patch offsets</li>
<li>Go to Step 2 until converge.</li>
</ol>
<h3 id="2-3D-reconstruction"><a href="#2-3D-reconstruction" class="headerlink" title="2 3D reconstruction"></a>2 3D reconstruction</h3><h4 id="2-1-3D-representations"><a href="#2-1-3D-representations" class="headerlink" title="2.1 3D representations"></a>2.1 3D representations</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/14.png" class>
<ul>
<li><p>点云</p>
</li>
<li><p>mesh 用G(E, V)表示</p>
</li>
<li><p>voxel</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/15.png" class>
</li>
<li><p>SDF(Signed Distance Function)</p>
<ul>
<li>The distance of a point to the shape boundary</li>
<li>The distance is defined by a metric, usually the Euclidean distance</li>
</ul>
<p>Truncated Signed Distance Function (TSDF): Truncation SDF’s distance value to [−1, 1]</p>
</li>
</ul>
<h4 id="2-2-3D-surface-reconstruction"><a href="#2-2-3D-surface-reconstruction" class="headerlink" title="2.2 3D surface reconstruction"></a>2.2 3D surface reconstruction</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qinqinxiansheng/article/details/119449196">KinectFusion</a></p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/16.png" class>
<p><strong><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/8641e0db0367">泊松重建</a></strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38060850/article/details/109143025"><strong>Marching Cubes算法</strong></a></p>
<p>视频介绍Marching Cubes算法: </p>
<iframe src="//player.bilibili.com/player.html?aid=79262663&bvid=BV1yJ411r73v&cid=135644481&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/whuawell/article/details/74998280">Marching Squares</a> 基本和Marching cubes 类似。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/17.png" class>
<p><strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/jiankangyq/article/details/121808174">COLMAP</a></strong>: 一种通用的运动结构 (SfM) 和多视图立体 (MVS) 管道。</p>
<h4 id="2-3-Texture-mapping"><a href="#2-3-Texture-mapping" class="headerlink" title="2.3 Texture mapping"></a>2.3 Texture mapping</h4><blockquote>
<p>Surface lives in 3D world space</p>
<p>Every 3D surface point also has a place where it goes in the 2D image (texture).</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/364045620">纹理映射(Texture mapping)</a></p>
<h2 id="Lec-9-Deep-Learning"><a href="#Lec-9-Deep-Learning" class="headerlink" title="Lec 9 Deep Learning"></a>Lec 9 Deep Learning</h2><h3 id="1-Machine-learning"><a href="#1-Machine-learning" class="headerlink" title="1 Machine learning"></a>1 Machine learning</h3><blockquote>
<p>传统程序是给电脑输入和程序，电脑给出输出。</p>
<p>机器学习是给电脑输入和输出，电脑给出程序。</p>
</blockquote>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li><p>Model: $x$和$y$之间关系的数学表示</p>
</li>
<li><p>Supervised learning(监督学习): 可以由训练资料中学到或建立一个模式（函数/learning model），并且依次模式推测出新的实例。</p>
<p>labeled data: exisitng (x,y) pairs, called training data.</p>
</li>
<li><p>机器学习的两个阶段：</p>
<ul>
<li>训练(Training)</li>
<li>测试(Testing)</li>
</ul>
</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/18.png" class>
<h3 id="2-Linear-classifier"><a href="#2-Linear-classifier" class="headerlink" title="2 Linear classifier"></a>2 Linear classifier</h3><h4 id="CLassification-model"><a href="#CLassification-model" class="headerlink" title="CLassification model"></a>CLassification model</h4><blockquote>
<p>输入是一张图片</p>
<p>输出是每个分类的对应分数</p>
</blockquote>
<p>有两部分组成：</p>
<ul>
<li>评分函数</li>
<li>损失函数</li>
</ul>
<h4 id="Linear-classifier"><a href="#Linear-classifier" class="headerlink" title="Linear classifier"></a>Linear classifier</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/19.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/20.png" class>
<p>将一张照片里面的所有像素变成一个向量。</p>
<p>$f(x_i,W,b) = Wx_i + b$</p>
<p>参数<strong>W</strong>被称为<strong>权重（weights）</strong>，<strong>b</strong>被称为<strong>偏差向量（bias vector）</strong>。</p>
<ul>
<li>首先，一个单独的矩阵乘法$Wx_i$就高效地并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。</li>
<li>训练数据用来学习$W$和$b$</li>
<li>一张图像可看做高维空间的一个点，每个分类就是把这些点划分成若干个区域。</li>
</ul>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><blockquote>
<p>判断一个权重矩阵是否足够好</p>
<p>回归问题使用均方误差(MSE)</p>
<p>分类问题使用交叉熵(Cross Entropy Loss)</p>
<p>参考资料：<a target="_blank" rel="noopener" href="https://blog.csdn.net/xg123321123/article/details/80781611">简单谈谈Cross Entropy Loss</a></p>
</blockquote>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/21.png" class>
<p>Softmax: 把K个实值转换为另外K个实值并使K个实值之和为1的函数。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/22.png" class>
<h3 id="3-Neural-networks"><a href="#3-Neural-networks" class="headerlink" title="3 Neural networks"></a>3 Neural networks</h3><blockquote>
<p>参考资料：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39910711/article/details/114849349">激活函数（Activation Function）</a></p>
</blockquote>
<p>​    <strong>激活函数</strong>：不使用激活函数的话，神经网络的每层都只是做<strong>线性变换</strong>，多层输入叠加后也还是线性变换。因为线性模型的表达能力通常不够，所以这时候就体现了激活函数的作用了，激活函数可以引入<strong>非线性因素</strong>。</p>
<p>​    在神经网络每一层神经元做完线性变换后，加上一个非线性激励函数对线性变换的结果进行转换，输出就可以变成一个非线性的函数。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/23.png" class>
<p><strong>多层感知器</strong></p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/24.png" class>
<p><strong>全连接神经网络</strong></p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/25.png" class>
<h3 id="4-Convolutional-neural-networks—卷积神经网络"><a href="#4-Convolutional-neural-networks—卷积神经网络" class="headerlink" title="4 Convolutional neural networks—卷积神经网络"></a>4 Convolutional neural networks—卷积神经网络</h3><blockquote>
<p>参考资料：<a target="_blank" rel="noopener" href="https://blog.csdn.net/jiaoyangwm/article/details/80011656/">卷积神经网络超详细介绍</a></p>
</blockquote>
<h4 id="Convolution-local-connectivity-weight-sharing"><a href="#Convolution-local-connectivity-weight-sharing" class="headerlink" title="Convolution = local connectivity + weight sharing"></a>Convolution = local connectivity + weight sharing</h4><blockquote>
<p>两者的关键作用就是减少参数数量，使运算变得简洁、高效，能够在超大规模数据集上运算</p>
</blockquote>
<p>local connectivity(局部连接): 对于局部连接而言：层间神经只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于去全连接减少了感受域外的连接，有效减少参数规模。</p>
<p>weight sharing(权值共享): 从图像局部学习到的信息应用到图像的其他部位去。权值共享意味着每一个过滤器在遍历整个图像的时候，过滤器的参数(即过滤器的参数的值)是固定不变的</p>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/malvas/article/details/86647781">weight sharing</a></p>
<h4 id="感受野-Receptive-fields"><a href="#感受野-Receptive-fields" class="headerlink" title="感受野(Receptive fields)"></a>感受野(Receptive fields)</h4><ul>
<li>若目标相对感受野过小，那训练参数只有少部分是对应于训练目标的，则在测试环节，也很难检测出类似的目标；</li>
<li>若目标相对感受野过大，那训练的参数都是对应于整个对象的局部信息，是不够利于检测小目标的。</li>
</ul>
<h4 id="池化层-Pooling-layer"><a href="#池化层-Pooling-layer" class="headerlink" title="池化层(Pooling layer)"></a>池化层(Pooling layer)</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/26.png" class>
<h4 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/27.png" class>
<h3 id="5-Training-neural-networks"><a href="#5-Training-neural-networks" class="headerlink" title="5 Training neural networks"></a>5 Training neural networks</h3><h4 id="梯度下降训练CNN"><a href="#梯度下降训练CNN" class="headerlink" title="梯度下降训练CNN"></a>梯度下降训练CNN</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/28.png" class>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit">反向传播-cs231n</a></p>
</blockquote>
<ol>
<li>Forward data through the network, get loss</li>
<li>Backprop to calculate the gradients</li>
<li>Update the parameters using the gradient</li>
<li>Go to step 1 if not converged</li>
</ol>
<h4 id="随机梯度下降法-SGD"><a href="#随机梯度下降法-SGD" class="headerlink" title="随机梯度下降法(SGD)"></a>随机梯度下降法(SGD)</h4><blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41803874/article/details/114016587">随机梯度下降详解</a></p>
</blockquote>
<p>仅计算一批随机采样图像上的损失和梯度。</p>
<h4 id="超参数-hyper-parameters"><a href="#超参数-hyper-parameters" class="headerlink" title="超参数(hyper-parameters)"></a>超参数(hyper-parameters)</h4><p>算法运行前需要决定的参数。</p>
<p>选择依据：</p>
<ol>
<li>Train for original model</li>
<li>Validate to find hyperparameters</li>
<li>Test to understand generalizability</li>
</ol>
<h4 id="过拟合-overfitting"><a href="#过拟合-overfitting" class="headerlink" title="过拟合(overfitting)"></a>过拟合(overfitting)</h4><p>把噪音点也拟合上了，过分依赖数据集。</p>
<p>防止：</p>
<ol>
<li><p>Cross validation(验证) and early stop</p>
</li>
<li><p>Regularization(正则化) or dropout</p>
<p><strong>正则化</strong>：在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。 </p>
<p><strong>dropout</strong>：在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征</p>
</li>
<li><p>Data augmentation(数据增强)</p>
<blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41679153">数据增强(Data Augmentation)</a></p>
</blockquote>
<p>​    为了获得更多的数据，我们只要对现有的数据集进行微小的改变。比如旋转（flips）、移位（translations）、旋转（rotations）等微小的改变。我们的网络会认为这是不同的图片。</p>
</li>
</ol>
<h4 id="批标准化-Batch-Normalization"><a href="#批标准化-Batch-Normalization" class="headerlink" title="批标准化(Batch Normalization)"></a>批标准化(Batch Normalization)</h4><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/guoyaohua/p/8724433.html">深入理解BN</a></p>
</blockquote>
<p>​    目的：Reduce internal covariate shift(<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/480425962">内部协变量偏移</a>)</p>
<p>​    神经网络的深度增加，每层特征值分布会逐渐的向激活函数的输出区间的上下两端（激活函数饱和区间）靠近，这样继续下去就会导致梯度消失。BN就是通过方法将<strong>该层特征值分布重新拉回标准正态分布</strong>，特征值将落在激活函数对于输入较为敏感的区间，输入的小变化可导致损失函数较大的变化，使得梯度变大，避免梯度消失，同时也可加快收敛。</p>
<p>​    训练时的使用方法：对每个隐层加上一层BN。</p>
<h3 id="6-Network-architectures"><a href="#6-Network-architectures" class="headerlink" title="6 Network architectures"></a>6 Network architectures</h3><blockquote>
<p>以前发展不好：</p>
<ul>
<li>数据集过小导致过拟合</li>
<li>计算能力不够</li>
</ul>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42076902/article/details/123864381">AlexNet</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45649076/article/details/120494328">ResNet</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44766883/article/details/112011420">DenseNet</a>：互相连接所有的层</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/29.png" class>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_47233366/article/details/123029998">MobileNets</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/fengmaomao1991/article/details/121247163">Neural Architecture Search</a>(神经架构搜索)</p>
<h2 id="Lec-10-Recognition"><a href="#Lec-10-Recognition" class="headerlink" title="Lec 10 Recognition"></a>Lec 10 Recognition</h2><h3 id="1-Semantic-segmentation-语义分割"><a href="#1-Semantic-segmentation-语义分割" class="headerlink" title="1 Semantic segmentation(语义分割)"></a>1 Semantic segmentation(语义分割)</h3><blockquote>
<p>在图像领域，语义指的是图像的内容，对图片意思的理解，比如左图的语义就是三个人骑着三辆自行车；分割的意思是从像素的角度分割出图片中的不同对象，对原图中的每个像素都进行标注。</p>
</blockquote>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/30.png" class>
<h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><p>滑动窗口：时间复杂度高，有限的感受野。</p>
<p>全连接卷积网络：一次做出预测，损失函数是每个像素的交叉熵。</p>
<p>Unpolling：一种上采样方法，有很多种具体案例。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/31.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/32.png" class>
<h4 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h4><p>Skip Connection: 跳过中间连接，使深层和浅层连接起来。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/33.png" class>
<h4 id="DeepLab"><a href="#DeepLab" class="headerlink" title="DeepLab"></a>DeepLab</h4><blockquote>
<p>参考: <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42137700/article/details/81835354">图像语义分割之FCN和CRF</a></p>
</blockquote>
<p>图像语义分割步骤：</p>
<ul>
<li>FCN - 全卷积网络</li>
<li>CRF - 条件随机场(Conditional random field)</li>
<li>MRF - 马尔科夫随机场</li>
</ul>
<h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><p>Per-pixel Intersection-over-union</p>
<h3 id="2-Object-detection-目标检测"><a href="#2-Object-detection-目标检测" class="headerlink" title="2 Object detection(目标检测)"></a>2 Object detection(目标检测)</h3><blockquote>
<p>输入：一张RGB图片</p>
<p>输出：表示对象的一组边界框(类别标签、框的位置，框的大小)</p>
</blockquote>
<h4 id="单个物体检测"><a href="#单个物体检测" class="headerlink" title="单个物体检测"></a>单个物体检测</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/34.png" class>
<h4 id="多个物体检测"><a href="#多个物体检测" class="headerlink" title="多个物体检测"></a>多个物体检测</h4><p>一张照片经过各种不同的神经网络，得出结果。</p>
<h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.</p>
<p>一张图片可以被拆分成很多boxes，我们不能检测所有这些图片。</p>
<h4 id="Region-proposals-候选区域"><a href="#Region-proposals-候选区域" class="headerlink" title="Region proposals(候选区域)"></a>Region proposals(候选区域)</h4><p>用图像分割算法先分割图像，然后再进行目标检测。</p>
<h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><ol>
<li>选建议框并调整尺寸</li>
<li>对每个类别使用SVM分类器进行打分</li>
<li>进行筛选</li>
<li>损失函数：$loU=\frac {Area Of Overlap}{Area Of Union}$</li>
</ol>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/35.png" class>
<p>Mean Average Precision (mAP)：</p>
<ol>
<li>Run object detector on all test images </li>
<li><p>For each category, compute Average Precision (AP) = area under Precision vs Recall Curve </p>
<ol>
<li><p>For each detection (highest score to lowest score) </p>
<ol>
<li><p>If it matches some GT box with IoU &gt; 0.5, mark it as positive and eliminate the GT</p>
</li>
<li><p>Otherwise mark it as negative </p>
</li>
<li><p>Plot a point on PR Curve</p>
</li>
</ol>
</li>
<li>Average Precision (AP) = area under PR curve</li>
</ol>
</li>
<li>Mean Average Precision (mAP) = average of AP for each category</li>
<li>For “COCO mAP”: Compute mAP@thresh for each IoU threshold (0.5, 0.55, 0.6, …, 0.95) and take average</li>
</ol>
<p>非最大抑制(Non-Max Suppression):</p>
<ol>
<li>Select the highest-scoring box </li>
<li>Eliminate lower-scoring boxes with IoU &gt; threshold </li>
<li>If any boxes remain, goto 1</li>
</ol>
<h4 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN:"></a>Fast R-CNN:</h4><p>A two-stage object detector</p>
<ul>
<li>First stage: run once per image<ul>
<li>Backbone network</li>
<li>RPN</li>
</ul>
</li>
<li>Second stage: run once per region<ul>
<li>Crop features: RoI pool / align</li>
<li>Predict object class</li>
<li>Predict bbox offset</li>
</ul>
</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/36.png" class>
<h4 id="Rol-Pool"><a href="#Rol-Pool" class="headerlink" title="Rol Pool"></a>Rol Pool</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41021141/article/details/120617660">R-CNN中的ROIPool、ROIAlign</a></p>
<h4 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h4><blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42912710/article/details/119872716">详解RPN网络</a></p>
<p>RPN（Region Proposal Network）是Faster-RCNN网络用于提取预选框（也就是RCNN中使用selective search算法进行Region Proposal的部分），R-CNN及Fast-RCNN中一个性能瓶颈就是提取预选框的部分，而RPN很好地对这个部分进行了优化，原因在于它将卷积神经网络引入了进来，使用特征提取的形式生成出预选框的位置从而降低了selective search算法带来的计算时间上的开销。</p>
</blockquote>
<p>​    假设我们有一张大小为600×800的图像，在通过卷积神经网络（CNN）块后，这幅输入图像缩小为38×56的特征图，特征图的每个位置都有9个锚点盒。那么我们就有38 <em> 56 </em> 9=1192个建议或锚箱需要考虑。而每个锚箱都有两个可能的标签（前景或背景）。如果我们把特征图的深度定为18（9个锚点x 2个标签），我们将使每个锚点都有一个有两个值的向量（通常称为预测值），代表前景和背景。如果我们将预测值送入softmax/logistic回归激活函数，它将预测标签。</p>
<h3 id="3-Instance-segmentation-实例分割"><a href="#3-Instance-segmentation-实例分割" class="headerlink" title="3 Instance segmentation(实例分割)"></a>3 Instance segmentation(实例分割)</h3><ol>
<li>语义分割的话只需要分出不同类就行，同类的不同个体不需要分，但是Instance Segmentation这里在语义分割的基础上又把不同的类进行了分割</li>
<li>目标检测后，需要对检测的部分做进一步的语义分割</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37392244/article/details/88844681">Mask R-CNN</a>:</p>
<p>在Faster R-CNN的基础上添加了一个预测分割mask的分支，如下图所示。其中黑色部分为原来的Faster-RCNN，红色部分为在Faster-RCNN网络上的修改。将RoI Pooling 层替换成了RoIAlign层；添加了并列的FCN层（mask层）。</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/37.png" class>
<blockquote>
<p>DeepSnake: 通过深度学习给出输入初始轮廓顶点需要调整的偏移量，以得到更为准确的实例分割结果。</p>
</blockquote>
<p>Panoptic segmentation: 对每一个像素都分出类别</p>
<h3 id="4-Human-pose-estimation-人体姿态估计"><a href="#4-Human-pose-estimation-人体姿态估计" class="headerlink" title="4 Human pose estimation(人体姿态估计)"></a>4 Human pose estimation(人体姿态估计)</h3><blockquote>
<p>通过定位一组关键点来表示人的姿势</p>
</blockquote>
<p>单人：Represent joint location as the heatmap(现在把关键点用热力图(heat map)表示，不需要全连接层，加速算法减少参数量。对于每一个关键点输出一张heat map)</p>
<p>多人：</p>
<ul>
<li>Top-down(准确)：把多人转成单人，图像分割</li>
<li>Bottom-up(快)：先检测出图中所有人的所有关键点，再对关键点进行分组，进而组装成多个人</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104917833">人体姿态估计(Human Pose Estimation)经典方法整理</a></p>
</blockquote>
<h3 id="5-Optical-flow-光流"><a href="#5-Optical-flow-光流" class="headerlink" title="5 Optical flow(光流)"></a>5 Optical flow(光流)</h3><blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41368247/article/details/82562165">计算机视觉—光流法(optical flow)简介</a></p>
<p>光流(optical flow)是空间运动物体在观察成像平面上的像素运动的瞬时速度。在时间间隔很小（比如视频的连续前后两帧之间）时，也等同于目标点的位移。</p>
</blockquote>
<h4 id="光流场："><a href="#光流场：" class="headerlink" title="光流场："></a>光流场：</h4><p>​    光流场是一个二维矢量场，它反映了图像上每一点灰度的变化趋势，可看成是带有灰度的像素点在图像平面上运动而产生的瞬时速度场。它包含的信息即是各像点的瞬时运动速度矢量信息。</p>
<p>​    研究光流场的目的就是为了从序列图像中近似计算不能直接得到的运动场。光流场在理想情况下，光流场对应于运动场。</p>
<h4 id="FlowNet"><a href="#FlowNet" class="headerlink" title="FlowNet"></a>FlowNet</h4><p>两部分：缩小和放大</p>
<ul>
<li>缩小(卷积)部分<ol>
<li>第一种缩小(卷积)方案是最朴素的方法的，就是将这一对图片的通道concat起来</li>
<li>第二中方案是这一对图片分开处理，分别进入卷积网路，得到各自的特征图，然后再找到它们特征图之间的联系。</li>
</ol>
</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/39.png" class>
<ul>
<li><p>放大部分</p>
<p>一边向后unconv，一边直接在小的特征图上预测，然后把结果双线性插值然后concat在unconv后的特征图上，然后接着往后传，重复四次后，得到的预测光流分辨率依然是输入的1/4，再重复之前的操作已没有太多提升，所以可以直接双线性插值得到和输入相同分辨率的光流预测图。</p>
</li>
</ul>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/38.png" class>
<h3 id="6-Other-tasks"><a href="#6-Other-tasks" class="headerlink" title="6 Other tasks"></a>6 Other tasks</h3><p>Video classification: Use 3D CNN</p>
<p>Temporal action localization: Generate proposals then classify</p>
<p>Multi-object tracking</p>
<h2 id="Lec-11-3D-Deep-Learning"><a href="#Lec-11-3D-Deep-Learning" class="headerlink" title="Lec 11  3D Deep Learning"></a>Lec 11  3D Deep Learning</h2><h3 id="1-Feature-matching"><a href="#1-Feature-matching" class="headerlink" title="1 Feature matching"></a>1 Feature matching</h3><h4 id="Super-Polint-用于提取特征点"><a href="#Super-Polint-用于提取特征点" class="headerlink" title="Super Polint: 用于提取特征点"></a>Super Polint: 用于提取特征点</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/40.png" class>
<ul>
<li><p>CNN-based detectors: Representing feature point locations by heatmaps</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/41.png" class>
</li>
<li><p>CNN-based descriptors: Extract descriptors from CNN feature maps(每一张图都是一层神经网络下的特征图，取所有图中对应的点做一个向量)</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/42.png" class>
</li>
</ul>
<h4 id="SuperGlue-现在最好的检测方法"><a href="#SuperGlue-现在最好的检测方法" class="headerlink" title="SuperGlue: 现在最好的检测方法"></a>SuperGlue: 现在最好的检测方法</h4><h3 id="2-Object-Pose-Estimation"><a href="#2-Object-Pose-Estimation" class="headerlink" title="2 Object Pose Estimation"></a>2 Object Pose Estimation</h3><p>Estimate the 3D location and orientation (位置和方向) of an object  realtive to the camera frame.</p>
<p>Before that, we need to define the geometry center of the object.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/43.png" class>
<ol>
<li>Find 3D-2D correspondences</li>
<li>Solve R and t by perspective-n-point (PnP) algorithm</li>
<li>Find 2D-3D correspondences: detecting keyponts using CNNs</li>
</ol>
<h3 id="3-Human-Pose-Estimation"><a href="#3-Human-Pose-Estimation" class="headerlink" title="3 Human Pose Estimation"></a>3 Human Pose Estimation</h3><h4 id="Markerless-MoCap-相机照射关键点，标记物在人身体上，Markless就是不用贴标记物"><a href="#Markerless-MoCap-相机照射关键点，标记物在人身体上，Markless就是不用贴标记物" class="headerlink" title="Markerless MoCap: 相机照射关键点，标记物在人身体上，Markless就是不用贴标记物"></a>Markerless MoCap: 相机照射关键点，标记物在人身体上，Markless就是不用贴标记物</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/44.png" class>
<h4 id="Monocular-3D-Human-Pose-Estimation-参数化人体模型"><a href="#Monocular-3D-Human-Pose-Estimation-参数化人体模型" class="headerlink" title="Monocular 3D Human Pose Estimation: 参数化人体模型"></a>Monocular 3D Human Pose Estimation: 参数化人体模型</h4><p>Estimating 3D human pose using a single camera</p>
<p>Using networks to regress joint locations</p>
<h3 id="4-Depth-Estimation"><a href="#4-Depth-Estimation" class="headerlink" title="4 Depth Estimation"></a>4 Depth Estimation</h3><blockquote>
<p>Multiview Depth Estimation: Reconstruct the dense 3D shape from a set of images and camera parameters</p>
</blockquote>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/45.png" class>
<p>Learned multi-view stereo</p>
<p>Cost volume is a 3D array that stores the errors of all pixels at all depths</p>
<h3 id="5-Single-Image-to-3D"><a href="#5-Single-Image-to-3D" class="headerlink" title="5 Single Image to 3D"></a>5 Single Image to 3D</h3><p>• Depth • Point Cloud • Mesh • Volume</p>
<h4 id="Monoculer-depth-estimation"><a href="#Monoculer-depth-estimation" class="headerlink" title="Monoculer depth estimation"></a>Monoculer depth estimation</h4><blockquote>
<p>Learning to guess depth from large-scale training data</p>
</blockquote>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/46.png" class>
<h4 id="Single-view-shape-estimation"><a href="#Single-view-shape-estimation" class="headerlink" title="Single-view shape estimation"></a>Single-view shape estimation</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/47.png" class>
<p>除了点云，mesh等方法也可应用。</p>
<h3 id="6-PointNet"><a href="#6-PointNet" class="headerlink" title="6 PointNet"></a>6 PointNet</h3><p>把点云放进神经网络，以前的神经网络都是光栅化的</p>
<p>挑战1：点顺序不确定——最后再做pooling</p>
<p>挑战2：位置变化不确定——估计姿态</p>
<h2 id="Lec-12-Computational-Photography-I"><a href="#Lec-12-Computational-Photography-I" class="headerlink" title="Lec 12 Computational Photography I"></a>Lec 12 Computational Photography I</h2><h3 id="1-High-Dynamic-Range-Imaging-HDR"><a href="#1-High-Dynamic-Range-Imaging-HDR" class="headerlink" title="1 High Dynamic Range Imaging (HDR)"></a>1 High Dynamic Range Imaging (HDR)</h3><h4 id="Exposure-曝光"><a href="#Exposure-曝光" class="headerlink" title="Exposure:曝光"></a>Exposure:曝光</h4><p><strong>Exposure = Gain(增益) x Irradiance(光线) x Time(时间)</strong></p>
<ul>
<li>Gain is controlled by the ISO </li>
<li>Irradiance is controlled by the aperture </li>
<li>Time is controlled by the shutter speed</li>
</ul>
<h4 id="Dynamic-Range"><a href="#Dynamic-Range" class="headerlink" title="Dynamic Range"></a>Dynamic Range</h4><p>The ratio between the largest and smallest values of a certain quantity.</p>
<h4 id="HDR"><a href="#HDR" class="headerlink" title="HDR"></a>HDR</h4><p>亮的地方和暗的地方能同时拍清楚</p>
<p>实现方法：</p>
<ul>
<li>Exposure bracketing: Capture multiple LDR images at different exposures(一次拍很多张)</li>
<li>Merging: Combine them into a single HDR image(然后合在一起)</li>
</ul>
<h4 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/48.png" class>
<h4 id="Tone-mapping"><a href="#Tone-mapping" class="headerlink" title="Tone mapping"></a>Tone mapping</h4><p>Display the HDR image (12-bit) on a SDR (standard dynamic range, 8-bit) device.</p>
<blockquote>
<p>Gamma compression</p>
</blockquote>
<p>X → aX^γ^, applied independently on R, G, B intend to keep more details on each parts.</p>
<ul>
<li><p>γ &lt; 1:  keep more datails on dart parts</p>
</li>
<li><p>γ &gt; 1: keep more datails on light parts</p>
</li>
</ul>
<h3 id="2-Deblurring"><a href="#2-Deblurring" class="headerlink" title="2 Deblurring"></a>2 Deblurring</h3><h4 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h4><ul>
<li>Defocus: the subject is not in the depth of view</li>
<li>Motion blur: moving subjects or unstable camera</li>
<li>……</li>
</ul>
<h4 id="Modeling-Image-Blur"><a href="#Modeling-Image-Blur" class="headerlink" title="Modeling Image Blur"></a>Modeling Image Blur</h4><p>The blurring process can be described by convolution. H is called blur kernel.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/49.png" class>
<p>Deblurring = Deconvolution</p>
<ul>
<li>NBID: Non-blind image deconvolution, the blur kernel is known.</li>
<li>BID: Blind image deconvolution, the blur kernel is also unknown.</li>
</ul>
<h4 id="NBID"><a href="#NBID" class="headerlink" title="NBID"></a>NBID</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/50.png" class>
<ul>
<li>G: The captured image (known) </li>
<li>F: Image to be solved (unknown) </li>
<li>H: Blur kernel (known)</li>
</ul>
<p>Inverse Filter: will also amplify noise</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/51.png" class>
<p>Wiener Filter: Suppress high frequency when reverse filtering</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/52.png" class>
<p>Deconvolution by optimization</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/53.png" class>
<p>Deconvolution is ill-posed, these are several sets of solutions have the same MSE.</p>
<p>Objective function = likelihood function + regular term</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/54.png" class>
<h4 id="BID"><a href="#BID" class="headerlink" title="BID"></a>BID</h4><p>Blur kernel is non-negative and sparse.</p>
<p>Optimized objective function:</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/55.png" class>
<h3 id="3-Colorization-黑白变彩色"><a href="#3-Colorization-黑白变彩色" class="headerlink" title="3 Colorization(黑白变彩色)"></a>3 Colorization(黑白变彩色)</h3><h4 id="Sample-based-colorization"><a href="#Sample-based-colorization" class="headerlink" title="Sample-based colorization"></a>Sample-based colorization</h4><p>把一张照片上的颜色迁移到另一张照片，主要任务是像素匹配</p>
<h4 id="Interactive-colorization"><a href="#Interactive-colorization" class="headerlink" title="Interactive colorization"></a>Interactive colorization</h4><p>给出线条大致的颜色(user-guided)</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/56.png" class>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/57.png" class>
<p>Constraint: User-specified colors of brushed pixels keep unchanged</p>
<h4 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network (GAN)"></a>Generative Adversarial Network (GAN)</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/58.png" class>
<p>D can be viewed as a loss function to train G：</p>
<ul>
<li>Called adversarial loss </li>
<li>Learned instead of being hand-designed </li>
<li>Can be applied to any image synthesis tasks</li>
</ul>
<h3 id="4-Super-Resolution"><a href="#4-Super-Resolution" class="headerlink" title="4 Super Resolution"></a>4 Super Resolution</h3><p>Super Resolution using GAN</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/59.png" class>
<h2 id="Lec-13-Computational-Photography-II"><a href="#Lec-13-Computational-Photography-II" class="headerlink" title="Lec 13 Computational Photography II"></a>Lec 13 Computational Photography II</h2><h3 id="1-Image-based-Rendering"><a href="#1-Image-based-Rendering" class="headerlink" title="1 Image-based Rendering"></a>1 Image-based Rendering</h3><p>Rendering: from 3D models to images</p>
<p>Image-based: 基于照片进行渲染</p>
<h4 id="Light-Fields"><a href="#Light-Fields" class="headerlink" title="Light Fields"></a>Light Fields</h4><p>The plenoptic function (7D) depicts light rays passing through.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/60.png" class>
<h4 id="Multi-Plane-Image-MPI"><a href="#Multi-Plane-Image-MPI" class="headerlink" title="Multi-Plane Image (MPI)"></a>Multi-Plane Image (MPI)</h4><p>A set of front-parallel planes at a fixed range of depths.</p>
<p>Each plane encodes an RGB color image and an alpha/transparency map α~d~.</p>
<img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/61.png" class>
<h4 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h4><img src="/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/62.png" class>
<h3 id="2-Neural-Rendering"><a href="#2-Neural-Rendering" class="headerlink" title="2 Neural Rendering"></a>2 Neural Rendering</h3><h4 id="Pose-Transfer-amp-Garment-Transfer"><a href="#Pose-Transfer-amp-Garment-Transfer" class="headerlink" title="Pose Transfer &amp; Garment Transfer"></a>Pose Transfer &amp; Garment Transfer</h4><blockquote>
<p>Input: Image(s) of a person</p>
<p>Output: Synthesised images of the persion in different poses (Pose Transfer),  or with different clothing (Garment Transfer).</p>
</blockquote>
<p>Method:</p>
<ul>
<li>Use parametric mesh (SMPL) to represent body pose and shape </li>
<li>Use high-dimensional UV texture map to encode appearance </li>
<li>Transfer the pose and appearance</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lhmd
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://lhmd.github.io/2022/11/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AF%BC%E8%AE%BA/" title="计算机视觉导论">https://lhmd.github.io/2022/11/10/计算机视觉导论/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh_CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/10/30/Complete-Binary-Search-Tree/" rel="prev" title="Complete Binary Search Tree">
                  <i class="fa fa-chevron-left"></i> Complete Binary Search Tree
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/11/18/NeRF/" rel="next" title="NeRF">
                  NeRF <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lhmd</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">112k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:23</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>
<script type="text/javascript" src="/js/click_love.js"></script>




    </div>
  </footer>

  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://unpkg.com/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://unpkg.com/pdfobject@2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>




  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://unpkg.com/mathjax@3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://unpkg.com/gitalk@1.8.0/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"lhmd","repo":"MyBlogtalk","client_id":"0698455a5fd3328d9d96","client_secret":"e36300e8cdbb37c3962cb94e0a1ce438a65971d3","admin_user":"lhmd","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://unpkg.com/gitalk@1.8.0/dist/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"4fa080333ea0ad53f6a73f608592f1b0"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>




</body>
</html>
